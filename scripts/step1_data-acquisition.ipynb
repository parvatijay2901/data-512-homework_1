{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLa9CeJ5zpF",
        "tags": []
      },
      "source": [
        "# Homework 1: Professionalism & Reproducibility\n",
        "## Data Acquisition\n",
        "\n",
        "The goal of this assignment is to construct, analyze, and publish a dataset of monthly article traffic for a selected set of pages from English Wikipedia from July 1, 2015 through September 30, 2024. We make sure to follow the best practices for open scientific research as mentioned in chapters \"Assessing Reproducibility\" and \"The Basic Reproducible Workflow Template\" of \"The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences\" publication.\n",
        "\n",
        "\n",
        "This notebook talks about \"Data Acquisition\". It is the first step in this assignemnt. Here we collect data from the Wikimedia Analytics API for articles related to rare diseases from July 01 2015 to September 30 2024 and then use the Pageviews API to obtain metrics for desktop, mobile web, and mobile app traffic based on a curated list of Wikipedia articles matched to rare diseases. The data collected are then organized into three JSON files: one for monthly mobile access, another for monthly desktop access, and a third for cumulative pageviews, with each file named according to the specified date range.\n",
        "\n",
        "Every step in this notebook is documented to ensure transparency and reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Import required Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bKW5Ntfs5zpL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# standard python modules\n",
        "import json, time, urllib.parse\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# not standard modules, need to be installed with pip/pip3 if not done earlier\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Get Article Names\n",
        "\n",
        "In this section, we obtain the disease names that will be used to make API requests in the next step. Let us start by loading a cleaned CSV file containing rare diseases ([provided with this assignment](https://drive.google.com/file/d/15_FiKhBgXB2Ch9c0gAGYzKjF0DBhEPlY/view?usp=drive_link)) into a DataFrame. From this, we extract the disease names from the 'disease' column to prepare for further API calls.\n",
        "\n",
        "This list of pages was collected by using a database of rare diseases maintained by the [National Organization for Rare Diseases (NORD)](https://rarediseases.org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>disease</th>\n",
              "      <th>pageid</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Klinefelter syndrome</td>\n",
              "      <td>19833554</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Klinefelter_synd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aarskog–Scott syndrome</td>\n",
              "      <td>7966521</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Aarskog–Scott_sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abetalipoproteinemia</td>\n",
              "      <td>68451</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Abetalipoprotein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MT-TP</td>\n",
              "      <td>20945466</td>\n",
              "      <td>https://en.wikipedia.org/wiki/MT-TP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ablepharon macrostomia syndrome</td>\n",
              "      <td>10776100</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Ablepharon_macro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           disease    pageid  \\\n",
              "0             Klinefelter syndrome  19833554   \n",
              "1           Aarskog–Scott syndrome   7966521   \n",
              "2             Abetalipoproteinemia     68451   \n",
              "3                            MT-TP  20945466   \n",
              "4  Ablepharon macrostomia syndrome  10776100   \n",
              "\n",
              "                                                 url  \n",
              "0  https://en.wikipedia.org/wiki/Klinefelter_synd...  \n",
              "1  https://en.wikipedia.org/wiki/Aarskog–Scott_sy...  \n",
              "2  https://en.wikipedia.org/wiki/Abetalipoprotein...  \n",
              "3                https://en.wikipedia.org/wiki/MT-TP  \n",
              "4  https://en.wikipedia.org/wiki/Ablepharon_macro...  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rare_diseases_df = pd.read_csv('../data/input_data/rare-disease_cleaned.AUG.2024.csv')\n",
        "rare_diseases_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Diseases:  1773\n",
            "Sample Disease Names:  ['18p', '18p-', '2006 in Africa', '2007 in Africa', '2009 swine flu pandemic vaccine', '21-Hydroxylase', '22q13 deletion syndrome', '3-M syndrome', '3-Methylglutaconic aciduria', 'AA amyloidosis']\n"
          ]
        }
      ],
      "source": [
        "rare_diseases_names = rare_diseases_df['disease'].tolist()  # get the list of diseases\n",
        "rare_diseases_names.sort()  # sorting helps users to quickly locate a specific disease name without having to scan through the long unsorted list\n",
        "print(\"Total Diseases: \", len(rare_diseases_names))\n",
        "print(\"Sample Disease Names: \", rare_diseases_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Define the constants\n",
        "\n",
        "This section is a snippet from the [example notebook provided with this assigment, revision dated: August 16, 2024](https://drive.google.com/file/d/1fYTIX79t9jk-Jske8IwysV-rbRkD4_dc/view?usp=drive_link) (licensed [CC-BY](https://www.google.com/url?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2F))\n",
        "\n",
        "The following constants are essential for making requests to the Wikimedia Pageviews API amd helps in making the code to be more readable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "AlAQOlOp5zpO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This variable holds the base URL for all pageviews API requests.\n",
        "# It serves as the starting point for constructing requests to gather pageview metrics for specific articles.\n",
        "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
        "\n",
        "# This is specific format for making per-article pageviews requests.\n",
        "# It uses placeholders that will be replaced with actual values when generating the final request URL.\n",
        "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
        "\n",
        "# To comply with the Wikimedia API's rate limiting policy, these constants define a slight delay between requests.\n",
        "# We assume roughly 2ms latency on the API and network\n",
        "# Throttle wait variable calculates the wait time between requests to ensure we do not exceed the limit of 100 requests per second.\n",
        "API_LATENCY_ASSUMED = 0.002\n",
        "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
        "\n",
        "# This dictionary contains headers that are included in each API request.\n",
        "# We need to providing an email address to be notified when there is a rate limit violations / other errors\n",
        "REQUEST_HEADERS = {\n",
        "    'User-Agent': '<pj2901@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024',\n",
        "}\n",
        "\n",
        "# We have the rare disease names that we extracted in the previous step here.\n",
        "# It will be used to iterate through and make requests for each specific article to collect pageview data.\n",
        "ARTICLE_TITLES = rare_diseases_names\n",
        "\n",
        "# This dictionary acts as a template for constructing the parameters for our API requests with a start date of 07/01/2015, and an end date of 09/30/2024.\n",
        "# Most fields remain constant across requests, such as project, agent, and granularity.\n",
        "# However, the article name and access type will be dynamically set before each request.\n",
        "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
        "    \"project\":     \"en.wikipedia.org\",\n",
        "    \"access\":      \"\",  # this value will be changed for the different access types\n",
        "    \"agent\":       \"user\",\n",
        "    \"article\":     \"\",  # this value will be set/changed before each request\n",
        "    \"granularity\": \"monthly\",\n",
        "    \"start\":       \"2015070100\",\n",
        "    \"end\":         \"2024093000\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Define required Functions\n",
        "\n",
        "This section contains user-defined functions for interacting with the Wikimedia Pageviews API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.1. request_pageviews_per_article\n",
        "\n",
        "This section is a snippet from the [example notebook provided with this assigment, revision dated: August 16, 2024](https://drive.google.com/file/d/1fYTIX79t9jk-Jske8IwysV-rbRkD4_dc/view?usp=drive_link) (licensed [CC-BY](https://www.google.com/url?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2F)) \n",
        "\n",
        "Here we construct and send REST API requests to the Wikimedia Pageviews API for pageview data associated with a specific article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vSWVMipl5zpQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def request_pageviews_per_article(article_title = None,\n",
        "                                  access_type = None,\n",
        "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT,\n",
        "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS,\n",
        "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
        "                                  headers = REQUEST_HEADERS):\n",
        "    \"\"\"\n",
        "    Fetches pageview data for a specified article and access type from the API.\n",
        "\n",
        "    Parameters:\n",
        "        article_title (str): The title of the article for which to fetch pageview data.\n",
        "        access_type (str): The type of access ('mobile-app', 'mobile-web', 'desktop') to filter the data.\n",
        "        endpoint_url (str): The URL of the API endpoint to send the request to.\n",
        "        endpoint_params (str): The URL parameters template for the API request.\n",
        "        request_template (dict): A template dictionary for formatting the request parameters.\n",
        "        headers (dict): The headers to include in the request.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If no article title or access type is supplied.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The JSON response containing the pageview data if successful, otherwise None.\n",
        "    \"\"\"\n",
        "\n",
        "    # article title can a parameter to the request_template call\n",
        "    if article_title:\n",
        "        request_template['article'] = article_title\n",
        "    if not request_template['article']:\n",
        "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
        "\n",
        "    # titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
        "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'), safe=' ')\n",
        "    request_template['article'] = article_title_encoded\n",
        "\n",
        "    # access type can be a parameter to the request_template call\n",
        "    if access_type:\n",
        "        request_template['access'] = access_type\n",
        "    if not request_template['access']:\n",
        "        raise Exception(\"Must supply an access type to make a pageviews request.\")\n",
        "\n",
        "    # create a request URL by combining the endpoint_url with the parameters for the request\n",
        "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
        "\n",
        "    # make the request\n",
        "    try:\n",
        "        if API_THROTTLE_WAIT > 0.0:\n",
        "            time.sleep(API_THROTTLE_WAIT) # throttling is always a good practice with a free community sources\n",
        "        response = requests.get(request_url, headers=headers)\n",
        "        json_response = response.json()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        json_response = None\n",
        "    return json_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. combine_mobile_views\n",
        "\n",
        "In this function, we combine the pageviews from both mobile app and mobile web sources for each article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine_mobile_views(mobile_app_view, mobile_web_view):\n",
        "    \"\"\"\n",
        "    Combines mobile app and mobile web pageview data into a single list of combined views.\n",
        "\n",
        "    Parameters:\n",
        "        mobile_app_view (dict): The pageview data from the mobile app.\n",
        "        mobile_web_view (dict): The pageview data from the mobile web.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If either mobile_app_view or mobile_web_view is missing.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing combined views for each article with their respective timestamps.\n",
        "    \"\"\"\n",
        "    # Make sure that we have the required views to append\n",
        "    if not mobile_app_view or not mobile_web_view:\n",
        "        raise Exception(\"Must have mobile app and web views to append.\")\n",
        "\n",
        "    # Combine the views and have it in a combined_view variable\n",
        "    combined_view = []\n",
        "    for app, web in zip(mobile_app_view['items'], mobile_web_view['items']):\n",
        "        combined_view.append({'article': app['article'],\n",
        "                            'timestamp': app['timestamp'],\n",
        "                            'views': app['views'] + web['views']})\n",
        "    return combined_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3 test_view_saved_JSON_file\n",
        "\n",
        "This function just tests if the JSON files are saved successfully and is in the format we need for further analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_view_saved_JSON_file(filename):\n",
        "    \"\"\"\n",
        "    This function takes a filename as input, constructs the full file path using the specified filename and date range.\n",
        "    It then opens the JSON file, loads its contents, and prints the data in a readable format.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The filename for the JSON file to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    start_date = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['start'][:6]\n",
        "    end_date = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['end'][:6]\n",
        "    file_path = f'../data/generated_data/rare-disease_{filename}_{start_date}-{end_date}.json'\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    print(json.dumps(data, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Generate the dataset\n",
        "\n",
        "In this section, we iterate through a predefined list of article titles, and get pageview data for each access type (desktop, mobile app, and mobile web). Later, we combine both the mobile views and store the whole dataset in three distinct JSON files for future analysis.\n",
        "\n",
        "We are asked to produce these files for further analysis:\n",
        "- **Monthly mobile access** - Since, the API separates mobile access types into two separate requests, we need to combine them and have a single mobile access data in the file in this format: `rare-disease_monthly_mobile_\\<startYYYYMM>-\\<endYYYYMM>.json`\n",
        "- **Monthly desktop access** - Monthly desktop page traffic is based on one single request. We should store the desktop access data in a file in this format: `rare-disease_monthly_desktop_\\<startYYYYMM>-\\<endYYYYMM>.json`\n",
        "- **Monthly cumulative** - Monthly cumulative data is the sum of all mobile, and all desktop traffic per article. We should store the monthly cumulative data in a file in the format: `rare-disease_monthly_cumulative_\\<startYYYYMM>-\\<endYYYYMM>.json`\n",
        "For all of the files the \\<startYYYYMM> and \\<endYYYYMM> represent the starting and ending year and month as integer text strings.\n",
        "\n",
        "The generated dataset is stored in the `data/data_generated/` folder to ensure better clarity and distinction from the original input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched 1773 rare disease articles!\n",
            "\n",
            "Monthly mobile access data has been successfully saved to a JSON file!\n",
            "Monthly desktop access data has been successfully saved to a JSON file!\n",
            "Monthly cumulative data has been successfully saved as a JSON file!\n"
          ]
        }
      ],
      "source": [
        "mobile_data = {}\n",
        "desktop_data = {}\n",
        "cumulative_data = {}\n",
        "\n",
        "for article in ARTICLE_TITLES:\n",
        "    # Fetch data for each access type\n",
        "    print(f\"\\nFetching data for article: {article}...\")\n",
        "    mobile_app_views = request_pageviews_per_article(article, 'mobile-app')\n",
        "    mobile_web_views = request_pageviews_per_article(article, 'mobile-web')\n",
        "    mobile_views = combine_mobile_views(mobile_app_views, mobile_web_views) # Combibe the mobile views\n",
        "\n",
        "    desktop_views = request_pageviews_per_article(article, 'desktop')\n",
        "\n",
        "    # Calculate cumulative views (desktop + mobile)\n",
        "    cumulative_views = []\n",
        "    for desktop, mobile in zip(desktop_views['items'], mobile_views):\n",
        "        cumulative_views.append({'article': desktop['article'],\n",
        "                                'timestamp': desktop['timestamp'],\n",
        "                                'views': desktop['views'] + mobile['views']})\n",
        "\n",
        "    # Store the data\n",
        "    mobile_data[article] = mobile_views\n",
        "    desktop_data[article] = desktop_views['items']\n",
        "    cumulative_data[article] = cumulative_views\n",
        "    print(f\"Data fetched successfully!\")\n",
        "\n",
        "clear_output(wait=True) # for better clarity, let us clear the previous outputs\n",
        "\n",
        "print(f\"Successfully fetched {len(ARTICLE_TITLES)} rare disease articles!\")    # Print the dataset fetch combined update\n",
        "\n",
        "# Save the data to JSON files\n",
        "start_date = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['start'][:6]\n",
        "end_date = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['end'][:6]\n",
        "\n",
        "with open(f'../data/generated_data/rare-disease_monthly_mobile_{start_date}-{end_date}.json', 'w') as mobile_file:\n",
        "    json.dump(mobile_data, mobile_file, indent=4)\n",
        "print(\"\\nMonthly mobile access data has been successfully saved to a JSON file!\")\n",
        "\n",
        "with open(f'../data/generated_data/rare-disease_monthly_desktop_{start_date}-{end_date}.json', 'w') as desktop_file:\n",
        "    json.dump(desktop_data, desktop_file, indent=4)\n",
        "print(\"Monthly desktop access data has been successfully saved to a JSON file!\")\n",
        "\n",
        "with open(f'../data/generated_data/rare-disease_monthly_cumulative_{start_date}-{end_date}.json', 'w') as cumulative_file:\n",
        "    json.dump(cumulative_data, cumulative_file, indent=4)\n",
        "print(\"Monthly cumulative data has been successfully saved as a JSON file!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oxnS9Uj5zpU"
      },
      "outputs": [],
      "source": [
        "# Test that the rare-disease_monthly_desktop file was saved successfully\n",
        "test_view_saved_JSON_file(\"monthly_desktop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test that the rare-disease_monthly_mobile file was saved successfully\n",
        "test_view_saved_JSON_file(\"monthly_mobile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test that the rare-disease_monthly_cumulative file was saved successfully\n",
        "test_view_saved_JSON_file(\"monthly_cumulative\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Conclusion\n",
        "\n",
        "As per the assignment requirements, we have successfully saved the JSON files: `rare-disease_monthly_mobile_\\<startYYYYMM>-\\<endYYYYMM>.json`, `rare-disease_monthly_desktop_\\<startYYYYMM>-\\<endYYYYMM>.json`, and `rare-disease_monthly_cumulative_\\<startYYYYMM>-\\<endYYYYMM>.json`.\n",
        "\n",
        "We will further use them in `step2_data-analysis.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
